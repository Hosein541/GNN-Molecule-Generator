{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GraphAlchemist: A Graph-Based Autoregressive VAE for Molecule Generation\n",
        "\n",
        "* **Author:** Hosein Mohammadi\n",
        "* **Date:** July 2024\n",
        "* **Contact:** [huseinmohammadi83@gmail.com](mailto:huseinmohammadi83@gmail.com)\n",
        "* **LinkedIn:** [Hosein Mohammadi](https://www.linkedin.com/in/hosein-mohammadi-979b8a2b2/)\n",
        "* **Project Repository:** [GNN-Molecule-Generator](https://github.com/Hosein541/GNN-Molecule-Generator)\n",
        "---\n",
        "\n",
        "### 📖 Project Overview\n",
        "\n",
        "This project explores the *de novo* design of novel molecules using a deep learning approach. It leverages a **graph-based representation** of chemical structures and implements an **Autoregressive Variational Autoencoder (VAE)** built with Graph Neural Networks (GNNs) to generate new, valid compounds.\n",
        "\n",
        "The model is trained on a dataset of molecules targeting the **EGFR protein**, sourced from the ChEMBL database. The ultimate goal is to learn the underlying patterns of this chemical space and generate novel molecules with desirable drug-like properties, evaluated using metrics like **QED (Quantitative Estimation of Drug-likeness)** and **SA Score (Synthetic Accessibility Score)**.\n",
        "\n",
        "### Key Features\n",
        "* **Graph-Based Representation:** Treats molecules as graphs, capturing rich structural information beyond simple string-based methods.\n",
        "* **GAT Encoder:** A Graph Attention Network (GAT) encodes molecules into a continuous latent space.\n",
        "* **Autoregressive Decoder:** A sophisticated decoder generates molecules step-by-step (atom-by-atom, bond-by-bond), leading to higher chemical validity.\n",
        "* **Constrained Generation:** The final generation script is \"smart,\" using chemical valence rules to guide the model and significantly improve the validity of outputs.\n",
        "* **Tech Stack:** Built with **PyTorch**, **PyTorch Geometric**, and the **RDKit** cheminformatics toolkit.\n",
        "\n",
        "### Project Workflow\n",
        "1.  **Data Collection:** Fetching and preprocessing data for the EGFR target from ChEMBL.\n",
        "2.  **Graph Conversion:** Transforming SMILES strings into graph data objects.\n",
        "3.  **Model Training:** Training the Autoregressive VAE using a checkpointing system.\n",
        "4.  **Molecule Generation:** Using the trained model to generate thousands of novel molecules.\n",
        "5.  **Analysis & Visualization:** Scoring, selecting, and visualizing the best candidates in 3D."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DNQBPJH4vMe"
      },
      "outputs": [],
      "source": [
        "# ===================================================================\n",
        "# Section 1: Library Installation\n",
        "# ===================================================================\n",
        "\n",
        "# For accessing the ChEMBL database\n",
        "!pip install -q chembl_webresource-client\n",
        "\n",
        "# Core chemoinformatics and GNN libraries\n",
        "!pip install -q rdkit-pypi\n",
        "!pip install -q torch_geometric\n",
        "\n",
        "# For 3D visualization\n",
        "!pip install -q py3Dmol\n",
        "\n",
        "# PyTorch is usually pre-installed in Colab, but this ensures it's there\n",
        "!pip install -q torch\n",
        "\n",
        "# ===================================================================\n",
        "# Section 2: Library Imports\n",
        "# ===================================================================\n",
        "\n",
        "# --- Standard Python Libraries ---\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# --- Core Data Science & Plotting Libraries ---\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Chemoinformatics Libraries (RDKit & ChEMBL) ---\n",
        "from chembl_webresource_client.new_client import new_client\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem, Descriptors, QED, RDConfig\n",
        "# Add SA_Score module to Python path for synthesis accessibility calculation\n",
        "sys.path.append(os.path.join(RDConfig.RDContribDir, 'SA_Score'))\n",
        "import sascorer\n",
        "\n",
        "# --- Deep Learning Libraries (PyTorch & PyTorch Geometric) ---\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "# The DataLoader for graph data comes from PyTorch Geometric\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch.nn import Sequential, Linear, ReLU, Module, ModuleList\n",
        "from torch_geometric.nn import GATConv, global_add_pool, global_mean_pool\n",
        "\n",
        "\n",
        "\n",
        "print(\"✅ All required libraries imported successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Data Collection & Preprocessing\n",
        "\n",
        "This initial step focuses on acquiring and cleaning the data needed to train our generative model. We source our data from the **ChEMBL database**, a large, open-access database of bioactive drug-like molecules.\n",
        "\n",
        "#### Data Fetching\n",
        "The process begins by targeting a specific protein, in this case, the **Epidermal Growth Factor Receptor (EGFR)**, which is identified by its ChEMBL ID `CHEMBL203`. We then fetch all associated bioactivity data, specifically looking for records that measure the **IC50** value. The IC50 is a common measure of a drug's potency, indicating the concentration of a drug required to inhibit a biological process by 50%.\n",
        "\n",
        "For each record, we retrieve the molecule's structure in **SMILES** format and its corresponding IC50 value.\n",
        "\n",
        "#### Data Cleaning & Transformation\n",
        "The raw data requires several preprocessing steps to be suitable for machine learning:\n",
        "* **Filtering:** We keep only the records with the \"IC50\" standard type to ensure consistency in our activity measurements.\n",
        "* **Cleaning:** Rows with missing SMILES strings or IC50 values are removed to ensure data quality.\n",
        "* **Calculating pIC50:** The IC50 values are typically on a logarithmic scale and can span many orders of magnitude. To make this data more tractable for the model, we convert the IC50 values (in nM) to **pIC50** values using the formula:\n",
        "    $$\n",
        "    \\text{pIC50} = -\\log_{10}(\\text{IC50}_{\\text{Molar}})\n",
        "    $$\n",
        "    A higher pIC50 value corresponds to a more potent molecule.\n",
        "\n",
        "Finally, the cleaned and processed dataset, containing the molecule's SMILES string and its pIC50 value, is saved to `egfr_ic50_dataset.csv`. This file serves as the primary input for the rest of the project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v61ieC2L4yqk"
      },
      "outputs": [],
      "source": [
        "# --- Settings ---\n",
        "target_id = \"CHEMBL203\"\n",
        "max_records = 30000\n",
        "\n",
        "activity = new_client.activity\n",
        "\n",
        "print(\"⏳ Fetching data from ChEMBL...\")\n",
        "\n",
        "# EGFR data generator\n",
        "activity_gen = activity.filter(\n",
        "    target_chembl_id=target_id\n",
        ").only(\n",
        "    [\n",
        "        \"canonical_smiles\",\n",
        "        \"standard_type\",\n",
        "        \"standard_value\",\n",
        "        \"molecule_chembl_id\",\n",
        "        \"activity_id\",\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Manually retrieve data up to max_records\n",
        "records = []\n",
        "for i, record in enumerate(activity_gen):\n",
        "    if i >= max_records:\n",
        "        break\n",
        "    records.append(record)\n",
        "    if i % 500 == 0:\n",
        "        print(f\"🔄 Received record {i}\")\n",
        "\n",
        "print(f\"🎯 Number of records received: {len(records)}\")\n",
        "\n",
        "# --- Convert to DataFrame and Preprocessing ---\n",
        "df = pd.DataFrame(records)\n",
        "\n",
        "# Filter IC50 and numerical values\n",
        "df = df[df[\"standard_type\"] == \"IC50\"]\n",
        "df = df[df[\"standard_value\"].notna()]\n",
        "df = df[df[\"canonical_smiles\"].notna()]\n",
        "df[\"standard_value\"] = pd.to_numeric(df[\"standard_value\"], errors=\"coerce\")\n",
        "df = df[df[\"standard_value\"] > 0]\n",
        "\n",
        "# Calculate pIC50\n",
        "df[\"pIC50\"] = -np.log10(df[\"standard_value\"] * 1e-9)\n",
        "\n",
        "# Final output\n",
        "final_df = df[[\"molecule_chembl_id\", \"canonical_smiles\", \"standard_value\", \"pIC50\"]]\n",
        "final_df.columns = [\"chembl_id\", \"smiles\", \"IC50_nM\", \"pIC50\"]\n",
        "final_df.to_csv(\"egfr_ic50_dataset.csv\", index=False)\n",
        "\n",
        "print(\"✅ File egfr_ic50_dataset.csv saved.\")\n",
        "print(f\"📦 Final records: {final_df.shape[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whtcxNOp43t9"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(\"egfr_ic50_dataset.csv\")\n",
        "\n",
        "# We only use the SMILES column for reconstruction\n",
        "smiles_list = df['smiles'].tolist()\n",
        "print(f\"Total SMILES count: {len(smiles_list)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Graph Representation: Converting SMILES to Graphs\n",
        "\n",
        "This code block is the cornerstone of our graph-based approach. Its primary purpose is to convert the entire dataset of **SMILES** strings into a format that a Graph Neural Network (GNN) can understand: a list of individual **graph** objects. This process is handled by the `smiles_to_graph` function.\n",
        "\n",
        "#### The Conversion Process\n",
        "\n",
        "For each molecule, the function constructs a graph where atoms are **nodes** and bonds are **edges**. To provide the GNN with rich chemical context, we extract a feature vector for every node and edge.\n",
        "\n",
        "* **Nodes (Atoms):** Each atom in the molecule becomes a node in the graph. Its feature vector contains the following chemical properties:\n",
        "    * Atomic Number\n",
        "    * Hybridization Type (e.g., sp, sp2, sp3)\n",
        "    * Total Number of Attached Hydrogens\n",
        "    * Formal Charge\n",
        "    * Aromaticity Status (True/False)\n",
        "    * Degree (total number of bonds)\n",
        "\n",
        "* **Edges (Bonds):** Each chemical bond between two atoms becomes an edge connecting the corresponding nodes. Its feature vector is a one-hot encoding that describes:\n",
        "    * Bond Type (Single, Double, Triple, or Aromatic)\n",
        "    * Whether the bond is part of a ring structure (True/False)\n",
        "\n",
        "#### Processing the Entire Dataset\n",
        "\n",
        "The main script iterates through every valid SMILES string from our cleaned dataset, applies this conversion function, and stores each resulting graph object in a list. A progress bar from `tqdm` visualizes this process.\n",
        "\n",
        "The final output is `egfr_graph_dataset.pt`, a single file containing the list of all processed graph objects. Saving the data in this serialized format allows for extremely fast loading in the subsequent model training steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QQiEEelSH3G"
      },
      "outputs": [],
      "source": [
        "# Define a function to convert a SMILES string to a graph\n",
        "def smiles_to_graph(smiles):\n",
        "    \"\"\"Converts a SMILES string to a PyTorch Geometric graph object.\"\"\"\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "\n",
        "    # --- Validation step ---\n",
        "    if mol is None:\n",
        "        return None # Return None if the molecule is invalid\n",
        "\n",
        "    mol = Chem.AddHs(mol)\n",
        "\n",
        "    # Extract node features (atoms)\n",
        "    node_features = []\n",
        "    for atom in mol.GetAtoms():\n",
        "        features = [\n",
        "            atom.GetAtomicNum(),\n",
        "            atom.GetHybridization(),\n",
        "            atom.GetTotalNumHs(),\n",
        "            atom.GetFormalCharge(),\n",
        "            atom.GetIsAromatic(),\n",
        "            atom.GetDegree(),\n",
        "        ]\n",
        "        node_features.append(features)\n",
        "    x = torch.tensor(node_features, dtype=torch.float)\n",
        "\n",
        "    # Extract edges and edge features (bonds)\n",
        "    if mol.GetNumBonds() == 0: # Molecule with no bonds (only one atom)\n",
        "        edge_index = torch.empty((2, 0), dtype=torch.long)\n",
        "        edge_attr = torch.empty((0, 5), dtype=torch.float)\n",
        "    else:\n",
        "        edge_indices, edge_features_list = [], []\n",
        "        for bond in mol.GetBonds():\n",
        "            i, j = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
        "            edge_indices.extend([(i, j), (j, i)])\n",
        "\n",
        "            bond_type = [\n",
        "                bond.GetBondType() == Chem.rdchem.BondType.SINGLE,\n",
        "                bond.GetBondType() == Chem.rdchem.BondType.DOUBLE,\n",
        "                bond.GetBondType() == Chem.rdchem.BondType.TRIPLE,\n",
        "                bond.GetBondType() == Chem.rdchem.BondType.AROMATIC,\n",
        "                bond.IsInRing(),\n",
        "            ]\n",
        "            edge_features_list.extend([bond_type, bond_type])\n",
        "\n",
        "        edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
        "        edge_attr = torch.tensor(edge_features_list, dtype=torch.float)\n",
        "\n",
        "    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr, smiles=smiles)\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('egfr_ic50_dataset.csv')\n",
        "\n",
        "# Remove rows without SMILES strings\n",
        "df.dropna(subset=['smiles'], inplace=True)\n",
        "\n",
        "\n",
        "# Create a list of graphs using the function and display a progress bar\n",
        "graph_list = []\n",
        "for smiles in tqdm(df['smiles'], desc=\"Processing Molecules\"):\n",
        "    try:\n",
        "        data = smiles_to_graph(smiles)\n",
        "        if data is not None:\n",
        "            graph_list.append(data)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing SMILES {smiles}: {e}\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Processing complete ---\")\n",
        "print(f\"Total molecules in the DataFrame: {len(df)}\")\n",
        "print(f\"Number of valid molecules converted to graphs: {len(graph_list)}\")\n",
        "\n",
        "# Now 'graph_list' is our ready dataset for the model.\n",
        "# You can save this list for later use.\n",
        "torch.save(graph_list, 'egfr_graph_dataset.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Model Architecture & Training\n",
        "\n",
        "This section contains the core of our project: the definition of our advanced generative model and the complex loop used to train it.\n",
        "\n",
        "#### Model Architecture 🧠\n",
        "\n",
        "The model is a **Variational Autoencoder (VAE)** specifically designed for graph data. It consists of two main components:\n",
        "\n",
        "1.  **Encoder (`GraphVAE_GAT`):**\n",
        "    This network's job is to read a complete molecular graph and compress it into a meaningful, low-dimensional vector in a \"latent space.\" It uses several **Graph Attention (GAT)** layers, which allow it to weigh the importance of different atoms and bonds to learn a highly informative representation of the molecule.\n",
        "\n",
        "2.  **Decoder (`AutoregressiveDecoder`):**\n",
        "    This is the generative part of the model. It takes a vector from the latent space and builds a new molecule step-by-step. This **autoregressive** process is powerful because each new decision is based on the state of the partially built molecule. At each step, the decoder makes three key predictions using separate MLP \"heads\":\n",
        "    * **Node Type:** What kind of atom should be added next? (e.g., Carbon, Nitrogen)\n",
        "    * **Edge Existence:** Should the new atom be connected to the existing atoms?\n",
        "    * **Edge Type:** If a connection is made, what kind of bond should it be? (e.g., Single, Double)\n",
        "\n",
        "#### The Training Process 🏋️‍♂️\n",
        "\n",
        "Training this model is more complex than a standard supervised task. We use a method called **Teacher Forcing**.\n",
        "\n",
        "During training, instead of letting the model use its own (potentially wrong) predictions to build the next step, we always provide it with the correct ground-truth structure from the dataset. This ensures the model learns the correct decision at every single step of molecule construction.\n",
        "\n",
        "The model's performance is measured by a **multi-task loss function**, which is a combination of:\n",
        "* **Node Type Loss:** Penalty for predicting the wrong atom type.\n",
        "* **Edge Existence Loss:** Penalty for incorrect bond predictions.\n",
        "* **Edge Type Loss:** Penalty for predicting the wrong bond type.\n",
        "* **KL Divergence Loss:** A standard VAE loss that helps organize the latent space, making it better for generating novel molecules.\n",
        "\n",
        "The final script initializes these models, sets up an Adam optimizer, and runs the training loop for a specified number of epochs, saving the model's progress along the way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKqtI1OWjoS3"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ===================================================================\n",
        "# Part 1: Definitions and Mappings (for code completeness)\n",
        "# ===================================================================\n",
        "ATOM_VOCAB = ['C', 'N', 'O', 'S', 'F', 'Cl', 'Br', 'I', 'P', 'B', 'H', 'Unknown']\n",
        "ATOM_MAP = {symbol: i for i, symbol in enumerate(ATOM_VOCAB)}\n",
        "NUM_ATOM_TYPES = len(ATOM_VOCAB)\n",
        "\n",
        "BOND_MAP = {\n",
        "    Chem.rdchem.BondType.SINGLE: 0,\n",
        "    Chem.rdchem.BondType.DOUBLE: 1,\n",
        "    Chem.rdchem.BondType.TRIPLE: 2,\n",
        "    Chem.rdchem.BondType.AROMATIC: 3,\n",
        "}\n",
        "NUM_BOND_TYPES = len(BOND_MAP)\n",
        "\n",
        "# ===================================================================\n",
        "# Part 2: Defining the Model Architectures (Encoder and Decoder)\n",
        "# ===================================================================\n",
        "\n",
        "class GraphVAE_GAT(Module):\n",
        "    def __init__(self, node_feature_dim, latent_dim=128, heads=4):\n",
        "        super(GraphVAE_GAT, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.encoder_conv1 = GATConv(node_feature_dim, 256, heads=heads)\n",
        "        self.encoder_conv2 = GATConv(256 * heads, 256, heads=heads)\n",
        "        self.encoder_conv3 = GATConv(256 * heads, latent_dim * 2, heads=1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        x = F.elu(self.encoder_conv1(x, edge_index))\n",
        "        x = F.elu(self.encoder_conv2(x, edge_index))\n",
        "        x = self.encoder_conv3(x, edge_index)\n",
        "        x = global_mean_pool(x, batch)\n",
        "        mu = x[:, :self.latent_dim]\n",
        "        log_var = x[:, self.latent_dim:]\n",
        "        return mu, log_var, x\n",
        "\n",
        "\n",
        "\n",
        "class AutoregressiveDecoder(Module):\n",
        "    def __init__(self, node_feature_dim, latent_dim, hidden_dim=128):\n",
        "        super(AutoregressiveDecoder, self).__init__()\n",
        "        self.gnn = GATConv(node_feature_dim + latent_dim, hidden_dim, heads=4)\n",
        "\n",
        "        mlp_input_dim = hidden_dim * 4\n",
        "\n",
        "        self.mlp_node_type = Sequential(\n",
        "            Linear(mlp_input_dim, hidden_dim), nn.ReLU(),\n",
        "            Linear(hidden_dim, NUM_ATOM_TYPES)\n",
        "        )\n",
        "        self.mlp_predict_edge = Sequential(\n",
        "            Linear(mlp_input_dim * 2, hidden_dim), nn.ReLU(),\n",
        "            Linear(hidden_dim, 1)\n",
        "        )\n",
        "        # ✅ NEW PREDICTION HEAD\n",
        "        self.mlp_edge_type = Sequential(\n",
        "            Linear(mlp_input_dim * 2, hidden_dim), nn.ReLU(),\n",
        "            Linear(hidden_dim, NUM_BOND_TYPES) # Predicts one of 4 bond types\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNDKgZFDj5rX"
      },
      "outputs": [],
      "source": [
        "def train_autoregressive_epoch(encoder, decoder, dataloader, optimizer, device):\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    total_epoch_loss = 0\n",
        "\n",
        "    for data_batch in tqdm(dataloader, desc=\"Training Epoch\"):\n",
        "        optimizer.zero_grad()\n",
        "        # ... (The first part of the loop is the same)\n",
        "        data_batch = data_batch.to(device)\n",
        "        try:\n",
        "            mu_batch, log_var_batch, _ = encoder(data_batch)\n",
        "        except (IndexError, TypeError) as e:\n",
        "            print(f\"Skipping batch due to an error: {e}\")\n",
        "            continue\n",
        "\n",
        "        std = torch.exp(0.5 * log_var_batch)\n",
        "        eps = torch.randn_like(std)\n",
        "        z_batch = mu_batch + eps * std\n",
        "\n",
        "        batch_loss_list = []\n",
        "        for i in range(data_batch.num_graphs):\n",
        "            # ... (The loop over molecules is the same)\n",
        "            z = z_batch[i]\n",
        "            true_graph = data_batch.get_example(i)\n",
        "            num_nodes = true_graph.num_nodes\n",
        "            if num_nodes <= 1: continue\n",
        "\n",
        "            partial_x = true_graph.x[0].unsqueeze(0)\n",
        "            partial_edge_index = torch.empty((2, 0), dtype=torch.long, device=device)\n",
        "\n",
        "            molecule_loss_list = []\n",
        "            for t in range(1, num_nodes):\n",
        "                # ... (GNN pass is the same)\n",
        "                z_expanded = z.repeat(t, 1)\n",
        "                gnn_input_x = torch.cat([partial_x, z_expanded], dim=1)\n",
        "                node_embeddings = F.elu(decoder.gnn(gnn_input_x, partial_edge_index))\n",
        "                graph_embedding = global_add_pool(node_embeddings, torch.zeros(t, dtype=torch.long, device=device))\n",
        "\n",
        "                # --- Loss Calculation ---\n",
        "                # 1. Node Type Loss (same as before)\n",
        "                true_node_type_idx = ATOM_MAP.get(Chem.Atom(int(true_graph.x[t, 0])).GetSymbol(), ATOM_MAP['Unknown'])\n",
        "                pred_node_type_logits = decoder.mlp_node_type(graph_embedding)\n",
        "                loss_node_type = F.cross_entropy(pred_node_type_logits, torch.tensor([true_node_type_idx], device=device))\n",
        "                molecule_loss_list.append(loss_node_type)\n",
        "\n",
        "                # 2. Edge Prediction Loss\n",
        "                if t > 0:\n",
        "                    new_node_emb = node_embeddings[-1].unsqueeze(0).repeat(t, 1)\n",
        "                    existing_node_embs = node_embeddings\n",
        "                    edge_mlp_input = torch.cat([new_node_emb, existing_node_embs], dim=1)\n",
        "\n",
        "                    # Edge Existence Loss (same as before)\n",
        "                    pred_edge_logits = decoder.mlp_predict_edge(edge_mlp_input).squeeze(-1)\n",
        "                    true_edges_exist = torch.zeros(t, device=device)\n",
        "                    for j in range(t):\n",
        "                        if ((true_graph.edge_index[0] == t) & (true_graph.edge_index[1] == j)).any():\n",
        "                            true_edges_exist[j] = 1.0\n",
        "                    loss_edge_exist = F.binary_cross_entropy_with_logits(pred_edge_logits, true_edges_exist)\n",
        "                    if not torch.isnan(loss_edge_exist):\n",
        "                        molecule_loss_list.append(loss_edge_exist)\n",
        "\n",
        "                    # ✅ NEW: Edge Type Loss\n",
        "                    # We only calculate this for bonds that actually exist\n",
        "                    true_edge_indices = (true_edges_exist == 1.0).nonzero().squeeze(-1)\n",
        "                    if true_edge_indices.numel() > 0:\n",
        "                        # Get embeddings for only the nodes that are truly connected\n",
        "                        connected_mlp_input = edge_mlp_input[true_edge_indices]\n",
        "                        pred_edge_type_logits = decoder.mlp_edge_type(connected_mlp_input)\n",
        "\n",
        "                        # Get the ground truth bond types\n",
        "                        true_bond_types = []\n",
        "                        for j in true_edge_indices:\n",
        "                            bond_mask = ((true_graph.edge_index[0] == t) & (true_graph.edge_index[1] == j))\n",
        "                            bond_type_feature_vector = true_graph.edge_attr[bond_mask][0]\n",
        "                            # bond_type_idx = bond_type_feature_vector.nonzero().item()\n",
        "                            bond_type_idx = bond_type_feature_vector[:4].nonzero().item()\n",
        "                            true_bond_types.append(bond_type_idx)\n",
        "\n",
        "                        loss_edge_type = F.cross_entropy(pred_edge_type_logits, torch.tensor(true_bond_types, device=device))\n",
        "                        if not torch.isnan(loss_edge_type):\n",
        "                            molecule_loss_list.append(loss_edge_type)\n",
        "\n",
        "                # --- Teacher Forcing (same as before) ---\n",
        "                partial_x = true_graph.x[:t+1]\n",
        "                edge_mask = (true_graph.edge_index[0] <= t) & (true_graph.edge_index[1] <= t)\n",
        "                partial_edge_index = true_graph.edge_index[:, edge_mask]\n",
        "\n",
        "            # ... (rest of the function is the same)\n",
        "            if molecule_loss_list:\n",
        "                batch_loss_list.append(torch.stack(molecule_loss_list).mean())\n",
        "        if batch_loss_list:\n",
        "            final_batch_loss = torch.stack(batch_loss_list).mean()\n",
        "            kl_loss = -0.5 * torch.sum(1 + log_var_batch - mu_batch.pow(2) - log_var_batch.exp())\n",
        "            final_batch_loss += (kl_loss / data_batch.num_graphs) * 0.001\n",
        "            final_batch_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(list(encoder.parameters()) + list(decoder.parameters()), 1.0)\n",
        "            optimizer.step()\n",
        "            total_epoch_loss += final_batch_loss.item()\n",
        "\n",
        "    return total_epoch_loss / len(dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbQw73_FSyKW"
      },
      "outputs": [],
      "source": [
        "# ===================================================================\n",
        "# Section 4: Final Initialization and Training Loop\n",
        "# ===================================================================\n",
        "# --- Model Parameters ---\n",
        "LATENT_DIM = 128\n",
        "HIDDEN_DIM = 128\n",
        "NODE_FEATURE_DIM = graph_list[0].num_node_features\n",
        "\n",
        "# --- Final Preparation ---\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- Build Models ---\n",
        "encoder = GraphVAE_GAT(node_feature_dim=NODE_FEATURE_DIM, latent_dim=LATENT_DIM).to(device)\n",
        "decoder = AutoregressiveDecoder(node_feature_dim=NODE_FEATURE_DIM, latent_dim=LATENT_DIM, hidden_dim=HIDDEN_DIM).to(device)\n",
        "\n",
        "# --- Optimizer ---\n",
        "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=0.0001)\n",
        "\n",
        "# --- Data Loader ---\n",
        "# ❗️ Tip: For initial testing, use a small subset\n",
        "small_graph_list = graph_list[:4096]\n",
        "data_loader = DataLoader(small_graph_list, batch_size=512, shuffle=True)\n",
        "# data_loader = DataLoader(graph_list, batch_size=16, shuffle=True)\n",
        "\n",
        "\n",
        "# --- Start Training ---\n",
        "NUM_EPOCHS = 2 # ❗️ For starting, test with a small number (e.g., 1 or 2)\n",
        "print(f\"Starting training for {NUM_EPOCHS} epochs...\")\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    start_time = time.time()\n",
        "    loss = train_autoregressive_epoch(encoder, decoder, data_loader, optimizer, device)\n",
        "    end_time = time.time()\n",
        "    print(f\"Epoch {epoch:02d} | Loss: {loss:.4f} | Time: {(end_time-start_time):.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Molecule Generation & Analysis\n",
        "\n",
        "This final section brings the entire project to its conclusion. Here, we use the fully trained autoregressive model to generate a large library of novel molecules. We then evaluate these generated molecules using standard chemoinformatics metrics to assess their quality and potential as drug candidates.\n",
        "\n",
        "#### The Generation Process\n",
        "\n",
        "The core of this step is the `generate_final_robust_molecule` function, which serves as the \"smart\" generator. It combines several key strategies to produce high-quality output:\n",
        "* **Autoregressive Construction:** It builds molecules step-by-step, making an intelligent decision at each stage based on the partially constructed graph.\n",
        "* **\"Adventurous\" Sampling:** To prevent the model from generating disconnected fragments (\"atomic soup\"), the function forces the new atom to connect to the `k` most probable existing atoms. This ensures the final output is a single, connected molecule.\n",
        "* **Valence Constraints:** Before adding any bond, the function performs a rigorous check against known chemical valence rules (e.g., Carbon's valence is 4). This significantly increases the rate of chemically valid outputs by preventing the formation of \"graph monsters.\"\n",
        "\n",
        "This process is repeated 1000 times to create a large and diverse pool of candidate molecules.\n",
        "\n",
        "#### Molecule Scoring\n",
        "\n",
        "After generation, every unique and valid molecule is passed to a scoring function. We evaluate each molecule based on two critical, industry-standard metrics:\n",
        "\n",
        "* **QED (Quantitative Estimation of Drug-likeness):** This metric scores a molecule on a scale from 0 to 1, based on how well its physicochemical properties align with those of known oral drugs. **A higher QED score is better.**\n",
        "* **SA Score (Synthetic Accessibility Score):** This score estimates how difficult it would be to synthesize the molecule in a lab. It ranges from 1 (very easy) to 10 (extremely difficult). **A lower SA Score is better.**\n",
        "\n",
        "The final output of this cell is a pandas DataFrame, `df_scores`, containing the SMILES strings of the successfully generated molecules and their corresponding QED and SA scores, ready for the final statistical analysis and visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9D5PVChGfDE"
      },
      "outputs": [],
      "source": [
        "# ===================================================================\n",
        "# Section 1: Required Dictionaries & Mappings\n",
        "# ===================================================================\n",
        "\n",
        "# --- Atom Vocab ---\n",
        "ATOM_VOCAB = ['C', 'N', 'O', 'S', 'F', 'Cl', 'Br', 'I', 'P', 'B', 'H', 'Unknown']\n",
        "ATOM_MAP = {symbol: i for i, symbol in enumerate(ATOM_VOCAB)}\n",
        "INV_ATOM_MAP = {i: symbol for symbol, i in ATOM_MAP.items()}\n",
        "\n",
        "# --- Bond Vocab & Mappings ---\n",
        "INV_BOND_TYPE_MAP = {\n",
        "    0: Chem.rdchem.BondType.SINGLE,\n",
        "    1: Chem.rdchem.BondType.DOUBLE,\n",
        "    2: Chem.rdchem.BondType.TRIPLE,\n",
        "    3: Chem.rdchem.BondType.AROMATIC\n",
        "}\n",
        "\n",
        "RDKIT_BOND_VALENCE_MAP = {\n",
        "    Chem.rdchem.BondType.SINGLE: 1.0,\n",
        "    Chem.rdchem.BondType.DOUBLE: 2.0,\n",
        "    Chem.rdchem.BondType.TRIPLE: 3.0,\n",
        "    Chem.rdchem.BondType.AROMATIC: 1.5\n",
        "}\n",
        "\n",
        "# --- Valence Constraints ---\n",
        "MAX_VALENCE = {\n",
        "    'C': 4, 'N': 3, 'O': 2, 'F': 1, 'Cl': 1, 'Br': 1, 'I': 1, 'S': 2,\n",
        "    'P': 3, 'B': 3, 'H': 1\n",
        "}\n",
        "\n",
        "# --- Model Parameters (ensure this matches your model's training) ---\n",
        "# Example: NODE_FEATURE_DIM = 6 (if you get it from graph_list[0].num_node_features)\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# Section 2: Final Robust Generation Function\n",
        "# ===================================================================\n",
        "\n",
        "def generate_final_robust_molecule(decoder, latent_vector, k=2, max_nodes=40, device='cpu'):\n",
        "    \"\"\"\n",
        "    Generates a single molecule using robust valence constraints and bond type predictions.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        # Data structures to build the molecule\n",
        "        nodes = [6] # Start with a Carbon atom (atomic number)\n",
        "        edges = [] # List of tuples (u, v, bond_type)\n",
        "\n",
        "        # Keep track of valences throughout generation\n",
        "        valences = torch.zeros(max_nodes, dtype=torch.float, device=device)\n",
        "\n",
        "        for t in range(1, max_nodes):\n",
        "            # Build the current partial graph for the model\n",
        "            partial_x = torch.zeros(t, NODE_FEATURE_DIM, device=device)\n",
        "            for i, atomic_num in enumerate(nodes):\n",
        "                partial_x[i, 0] = atomic_num\n",
        "\n",
        "            if not edges:\n",
        "                partial_edge_index = torch.empty((2, 0), dtype=torch.long, device=device)\n",
        "            else:\n",
        "                edge_tens = torch.tensor([(u,v) for u,v,bt in edges], device=device).t()\n",
        "                partial_edge_index = torch.cat([edge_tens, edge_tens.flip(0)], dim=1)\n",
        "\n",
        "            # Get Predictions from the Decoder\n",
        "            z_expanded = latent_vector.repeat(t, 1)\n",
        "            gnn_input_x = torch.cat([partial_x, z_expanded], dim=1)\n",
        "            node_embeddings = F.elu(decoder.gnn(gnn_input_x, partial_edge_index))\n",
        "            graph_embedding = global_add_pool(node_embeddings, torch.zeros(t, dtype=torch.long, device=device))\n",
        "\n",
        "            pred_node_type_logits = decoder.mlp_node_type(graph_embedding)\n",
        "            new_node_type_idx = torch.multinomial(F.softmax(pred_node_type_logits, dim=-1), 1).squeeze().item()\n",
        "\n",
        "            if new_node_type_idx == ATOM_MAP['Unknown']:\n",
        "                break\n",
        "\n",
        "            new_atom_symbol = INV_ATOM_MAP[new_node_type_idx]\n",
        "            nodes.append(Chem.Atom(new_atom_symbol).GetAtomicNum())\n",
        "            max_val_new = MAX_VALENCE.get(new_atom_symbol, 100)\n",
        "\n",
        "            if t > 0 and t < max_nodes:\n",
        "                new_node_emb = node_embeddings[-1].unsqueeze(0).repeat(t, 1)\n",
        "                edge_mlp_input = torch.cat([new_node_emb, node_embeddings], dim=1)\n",
        "\n",
        "                pred_edge_logits = decoder.mlp_predict_edge(edge_mlp_input).squeeze(-1)\n",
        "                edge_probs = torch.sigmoid(pred_edge_logits)\n",
        "\n",
        "                num_to_connect = min(k, t)\n",
        "                top_k_indices = torch.topk(edge_probs, k=num_to_connect).indices\n",
        "\n",
        "                top_k_mlp_input = edge_mlp_input[top_k_indices]\n",
        "                pred_edge_type_logits = decoder.mlp_edge_type(top_k_mlp_input)\n",
        "                bond_type_indices = torch.multinomial(F.softmax(pred_edge_type_logits, dim=-1), 1).squeeze(-1)\n",
        "\n",
        "                for i, j in enumerate(top_k_indices):\n",
        "                    j = j.item()\n",
        "                    bond_type_idx = bond_type_indices[i].item() if bond_type_indices.dim() > 0 else bond_type_indices.item()\n",
        "                    rdkit_bond_type = INV_BOND_TYPE_MAP.get(bond_type_idx, Chem.rdchem.BondType.SINGLE)\n",
        "                    bond_valence = RDKIT_BOND_VALENCE_MAP.get(rdkit_bond_type, 1.0)\n",
        "\n",
        "                    atom_j_symbol = Chem.Atom(nodes[j]).GetSymbol()\n",
        "                    max_val_j = MAX_VALENCE.get(atom_j_symbol, 100)\n",
        "\n",
        "                    if valences[j] + bond_valence <= max_val_j and valences[t] + bond_valence <= max_val_new:\n",
        "                        edges.append((t, j, rdkit_bond_type))\n",
        "                        valences[t] += bond_valence\n",
        "                        valences[j] += bond_valence\n",
        "\n",
        "    # --- Final step: Convert the collected nodes and edges to SMILES ---\n",
        "    mol = Chem.RWMol()\n",
        "    for atomic_num in nodes:\n",
        "        mol.AddAtom(Chem.Atom(atomic_num))\n",
        "\n",
        "    for u, v, bond_type in edges:\n",
        "        mol.AddBond(u, v, bond_type)\n",
        "\n",
        "    try:\n",
        "        Chem.SanitizeMol(mol)\n",
        "        smi = Chem.MolToSmiles(mol)\n",
        "        # Final check for disconnected fragments\n",
        "        if '.' in smi:\n",
        "            return \"Disconnected\"\n",
        "        return smi\n",
        "    except Exception:\n",
        "        return \"Invalid\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_KLSvL7vIt2"
      },
      "outputs": [],
      "source": [
        "# Ensure your trained models are loaded and in eval() mode\n",
        "# Ensure all helper functions and maps are defined\n",
        "decoder.eval()\n",
        "encoder.eval()\n",
        "print(\"🧪 Generating 1000 molecules...\")\n",
        "generated_smiles = []\n",
        "for i in tqdm(range(1000), desc=\"Generating Molecules\"):\n",
        "    random_z = torch.randn(1, LATENT_DIM).to(DEVICE)\n",
        "    # Using k=2 or k=3 can sometimes create more complex molecules\n",
        "    smiles = generate_final_robust_molecule(decoder, random_z, k=2, device=DEVICE)\n",
        "\n",
        "    # We only keep valid, single-fragment molecules\n",
        "    if smiles not in [\"Invalid\", \"Disconnected\"]:\n",
        "        generated_smiles.append(smiles)\n",
        "\n",
        "# Remove duplicates\n",
        "unique_smiles = list(set(generated_smiles))\n",
        "print(f\"\\n✅ Generated {len(unique_smiles)} unique, valid molecules.\")\n",
        "\n",
        "\n",
        "scores = []\n",
        "for smi in tqdm(unique_smiles, desc=\"Scoring Molecules\"):\n",
        "    mol = Chem.MolFromSmiles(smi)\n",
        "    if mol:\n",
        "        qed_score = QED.qed(mol)\n",
        "        sa_score = sascorer.calculateScore(mol)\n",
        "        scores.append({'smiles': smi, 'qed': qed_score, 'sa_score': sa_score})\n",
        "\n",
        "# Create a DataFrame for easy analysis\n",
        "df_scores = pd.DataFrame(scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Final Analysis & Visualization\n",
        "\n",
        "This is the final and most rewarding step of the project. After generating a large pool of molecules, we now analyze them quantitatively to identify the most promising candidates and visualize the best result.\n",
        "\n",
        "#### Selecting the Best Candidates 🏆\n",
        "\n",
        "To rank the generated molecules, we define a custom **`combined_score`**. This score is designed to find an optimal balance between drug-likeness and ease of synthesis. The formula used is:\n",
        "\n",
        "`combined_score = 2 * QED - (SA_Score / 10)`\n",
        "\n",
        "This formula prioritizes molecules with a high **QED** (multiplied by 2 for emphasis) while penalizing those with a high **SA Score**. The script first identifies the single best molecule based on this score and then selects the top 100 candidates for a more detailed statistical analysis.\n",
        "\n",
        "#### Statistical and Visual Analysis 📊\n",
        "\n",
        "To understand the properties of the high-quality molecules our model generated, the code produces a comprehensive analysis of the top 100 candidates:\n",
        "\n",
        "* **Descriptive Statistics:** A table is printed, summarizing key statistics like the mean, median, standard deviation, and quartiles for the QED, SA Score, and combined scores. This provides a quantitative snapshot of the model's performance.\n",
        "* **Visual Plots:** Three plots are generated to visualize the distribution and relationship of these properties:\n",
        "    1.  **QED Distribution:** Shows the range and frequency of drug-likeness scores among the top molecules.\n",
        "    2.  **SA Score Distribution:** Shows the distribution of synthetic accessibility scores.\n",
        "    3.  **QED vs. SA Scatter Plot:** Visualizes the trade-off between the two key metrics for the entire set of top candidates.\n",
        "\n",
        "#### 3D Visualization of the Top Molecule 🔬\n",
        "\n",
        "As a final step, the single best molecule is rendered in 3D. The script uses **RDKit** to:\n",
        "1.  Add hydrogen atoms to the 2D structure.\n",
        "2.  Embed the molecule into 3D space to generate realistic coordinates.\n",
        "3.  Optimize the 3D geometry using the UFF (Universal Force Field) algorithm.\n",
        "\n",
        "The resulting 3D structure is then displayed using **py3Dmol**, providing an interactive view of the novel molecule designed by our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivnUWbbpvmDm"
      },
      "outputs": [],
      "source": [
        "# We want to maximize this score (high QED, low SA Score)\n",
        "# Dividing sa_score by 10 helps normalize its scale relative to QED\n",
        "df_scores['combined_score'] = 2* df_scores['qed'] - (df_scores['sa_score'] / 10)\n",
        "\n",
        "# Find the molecule with the best combined score\n",
        "best_molecule_row = df_scores.sort_values(by='combined_score', ascending=False).iloc[0]\n",
        "best_smiles = best_molecule_row['smiles']\n",
        "print(\"\\n🏆 Best molecule found:\")\n",
        "print(best_molecule_row)\n",
        "\n",
        "\n",
        "df_top100 = df_scores.sort_values(by='combined_score', ascending=False).head(100)\n",
        "\n",
        "print(\"\\n\\n\\n🏆 Top 100 molecules selected. Here's a preview:\")\n",
        "print(df_top100.head())\n",
        "\n",
        "\n",
        "print(\"\\n\\n\\n📊 Descriptive Statistics for the Top 50 Generated Molecules:\")\n",
        "\n",
        "# The .describe() method calculates count, mean, std, min,\n",
        "# 25th percentile (Q1), 50th percentile (median), 75th percentile (Q3), and max.\n",
        "descriptive_stats = df_top100[['qed', 'sa_score', 'combined_score']].describe()\n",
        "\n",
        "# Display the results\n",
        "print(descriptive_stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "992ExI6Y4CDC"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Plot 1: QED Distribution\n",
        "axes[0].hist(df_top100['qed'], bins=30, color='tab:blue')\n",
        "axes[0].axvline(df_top100['qed'].mean(), color='red', linestyle='--', label=f\"Mean: {df_top100['qed'].mean():.2f}\")\n",
        "axes[0].set_title('QED Distribution (Top 100)')\n",
        "axes[0].set_xlabel('QED (higher is better)')\n",
        "axes[0].set_ylabel('Count')\n",
        "\n",
        "# Plot 2: SA Score Distribution\n",
        "axes[1].hist(df_top100['sa_score'], bins=20, color='tab:blue')\n",
        "axes[1].axvline(df_top100['sa_score'].mean(), color='red', linestyle='--', label=f\"Mean: {df_top100['sa_score'].mean():.2f}\")\n",
        "axes[1].set_title('SA Score Distribution (Top 100)')\n",
        "axes[1].set_xlabel('SA (lower = easier)')\n",
        "axes[1].set_ylabel('Count')\n",
        "\n",
        "# Plot 3: QED vs SA Score\n",
        "axes[2].scatter(df_top100['qed'], df_top100['sa_score'], alpha=0.5)\n",
        "axes[2].axvline(df_top100['qed'].mean(), color='red', linestyle='--')\n",
        "axes[2].axhline(df_top100['sa_score'].mean(), color='red', linestyle='--')\n",
        "axes[2].set_title('QED vs SA Score (Top 100)')\n",
        "axes[2].set_xlabel('QED (higher is better)')\n",
        "axes[2].set_ylabel('SA (lower is better)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVgSy_hlvsVc"
      },
      "outputs": [],
      "source": [
        "# Create molecule object and add hydrogens\n",
        "mol = Chem.MolFromSmiles(best_smiles)\n",
        "mol_with_hs = Chem.AddHs(mol)\n",
        "\n",
        "# Generate 3D coordinates\n",
        "AllChem.EmbedMolecule(mol_with_hs, randomSeed=42)\n",
        "AllChem.UFFOptimizeMolecule(mol_with_hs)\n",
        "\n",
        "# --- Visualization ---\n",
        "p = py3Dmol.view(width=500, height=500)\n",
        "\n",
        "# Convert RDKit mol to MOL block format\n",
        "mblock = Chem.MolToMolBlock(mol_with_hs)\n",
        "p.addModel(mblock, 'mol')\n",
        "\n",
        "# Style the view\n",
        "p.setStyle({'stick':{}})\n",
        "p.setBackgroundColor('0xeeeeee')\n",
        "p.zoomTo()\n",
        "print(\"\\n🔬 3D view of the best generated molecule:\")\n",
        "p.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Conclusion & Final Summary\n",
        "\n",
        "This project successfully developed a complete pipeline for **graph-based *de novo* molecule design** using an advanced deep learning architecture. By representing molecules as graphs, we were able to capture rich structural information, moving beyond traditional string-based methods.\n",
        "\n",
        "The core of the project is an **Autoregressive Variational Autoencoder (VAE)**. The **GAT-based Encoder** effectively learned to map complex molecules to a continuous latent space, while the **Autoregressive Decoder** demonstrated its ability to generate new graphs step-by-step.\n",
        "\n",
        "The development process was iterative and highlighted key challenges in generative chemistry. We progressed from simple models that produced invalid structures to more sophisticated ones that generated syntactically valid but chemically unrealistic molecules (\"graph monsters\"). The final breakthrough was achieved by combining our trained neural network with a **\"smart\" constrained generation algorithm**. This hybrid approach, which enforces chemical rules like valence during generation, proved essential for producing valid and connected structures.\n",
        "\n",
        "#### Key Achievements\n",
        "* A robust and debugged **autoregressive generative model** was successfully implemented and trained.\n",
        "* The model demonstrated the ability to generate **novel, valid, and connected** molecules.\n",
        "* A final analysis using metrics like **QED** and **SA Score** quantitatively confirmed the model's capacity to produce drug-like and synthetically accessible candidates.\n",
        "\n",
        "#### Future Directions\n",
        "This work serves as a strong foundation for more advanced research. The clear next steps are:\n",
        "1.  **Extended Pre-training:** Continuing the supervised training on a larger dataset and for more epochs to further improve the model's understanding of complex chemical patterns.\n",
        "2.  **Reinforcement Learning (RL):** Implementing an RL fine-tuning loop to optimize the generation process toward specific, user-defined objectives, such as maximizing the docking score against a protein target or achieving a specific property profile.\n",
        "\n",
        "Overall, this project provides a complete and powerful framework for modern, AI-driven drug discovery."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
